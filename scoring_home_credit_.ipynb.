{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 1. Download and Extract Dataset\n",
    "url = 'https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/home-credit-indonesia/home-credit-default-risk.zip'\n",
    "output_path = 'home-credit-default-risk.zip'\n",
    "response = requests.get(url)\n",
    "with open(output_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('home_credit_data')\n",
    "print(os.listdir('home_credit_data'))\n",
    "\n",
    "# 2. Load Data\n",
    "train_data = pd.read_csv('home_credit_data/application_train.csv')\n",
    "bureau = pd.read_csv('home_credit_data/bureau.csv')\n",
    "\n",
    "# 3. Data Exploration\n",
    "print('Application Train Info:')\n",
    "print(train_data.info())\n",
    "print('\\nBureau Info:')\n",
    "print(bureau.info())\n",
    "print('\\nMissing Values (Application Train):')\n",
    "print(train_data.isnull().mean().sort_values(ascending=False)[:10])\n",
    "print('\\nMissing Values (Bureau):')\n",
    "print(bureau.isnull().mean().sort_values(ascending=False)[:10])\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='TARGET', data=train_data)\n",
    "plt.title('Distribusi Kelas TARGET')\n",
    "plt.savefig('target_distribution.png')\n",
    "plt.show()\n",
    "numeric_cols = train_data.select_dtypes(include=np.number).columns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(train_data[numeric_cols].corr(), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Korelasi Fitur Numerik')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Data Cleaning & Feature Engineering\n",
    "train_data['CREDIT_INCOME_RATIO'] = train_data['AMT_CREDIT'] / train_data['AMT_INCOME_TOTAL']\n",
    "train_data['ANNUITY_INCOME_RATIO'] = train_data['AMT_ANNUITY'] / train_data['AMT_INCOME_TOTAL']\n",
    "train_data['DAYS_EMPLOYED'] = train_data['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg({\n",
    "    'CREDIT_ACTIVE': lambda x: (x == 'Active').sum(),\n",
    "    'AMT_CREDIT_SUM': ['mean', 'sum'],\n",
    "    'DAYS_CREDIT': ['mean', 'min']\n",
    "}).reset_index()\n",
    "bureau_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in bureau_agg.columns]\n",
    "train_data = train_data.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "numeric_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
    "train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].median())\n",
    "train_data[categorical_cols] = train_data[categorical_cols].fillna('Missing')\n",
    "selected_cols = train_data.columns[train_data.isnull().mean() < 0.4]\n",
    "train_data = train_data[selected_cols]\n",
    "\n",
    "# 5. Preprocessing\n",
    "X = train_data.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "y = train_data['TARGET']\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_processed, y)\n",
    "pd.DataFrame(X_resampled).to_csv('processed_data.csv', index=False)\n",
    "pd.Series(y_resampled).to_csv('processed_target.csv', index=False)\n",
    "\n",
    "# 6. Feature Importance\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf.fit(X_resampled, y_resampled)\n",
    "feature_names = (numeric_cols.tolist() + \n",
    "                 preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols).tolist())\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_names)\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.sort_values(ascending=False)[:10].plot.bar()\n",
    "plt.title('Top 10 Fitur Paling Penting')\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 7. Modeling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr_params = {'C': [0.01, 0.1, 1], 'penalty': ['l2'], 'solver': ['lbfgs']}\n",
    "lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20], 'min sÃ¥ntainer_split': [2, 5]}\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluation\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'--- {model_name} ---')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred):.2f}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred):.2f}')\n",
    "    print(f'F1-Score: {f1_score(y_test, y_pred):.2f}')\n",
    "    print(f'AUC-ROC: {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]):.2f}')\n",
    "evaluate_model(lr_grid.best_estimator_, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf_grid.best_estimator_, X_test, y_test, 'Random Forest')\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': accuracy_score(y_test, lr_grid.best_estimator_.predict(X_test)),\n",
    "        'Precision': precision_score(y_test, lr_grid.best_estimator_.predict(X_test)),\n",
    "        'Recall': recall_score(y_test, lr_grid.best_estimator_.predict(X_test)),\n",
    "        'F1-Score': f1_score(y_test, lr_grid.best_estimator_.predict(X_test)),\n",
    "        'AUC-ROC': roc_auc_score(y_test, lr_grid.best_estimator_.predict_proba(X_test)[:, 1])\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': accuracy_score(y_test, rf_grid.best_estimator_.predict(X_test)),\n",
    "        'Precision': precision_score(y_test, rf_grid.best_estimator_.predict(X_test)),\n",
    "        'Recall': recall_score(y_test, rf_grid.best_estimator_.predict(X_test)),\n",
    "        'F1-Score': f1_score(y_test, rf_grid.best_estimator_.predict(X_test)),\n",
    "        'AUC-ROC': roc_auc_score(y_test, rf_grid.best_estimator_.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "}\n",
    "pd.DataFrame(results).to_csv('model_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
